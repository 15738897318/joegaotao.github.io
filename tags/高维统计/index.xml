<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>高维统计 on Tao Gao | 高涛</title>
    <link>/tags/%E9%AB%98%E7%BB%B4%E7%BB%9F%E8%AE%A1/</link>
    <description>Recent content in 高维统计 on Tao Gao | 高涛</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>高涛 版权所有 2013 - 2017</copyright>
    <lastBuildDate>Sat, 10 Aug 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/%E9%AB%98%E7%BB%B4%E7%BB%9F%E8%AE%A1/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>PKU暑期高维统计学习心得(II)</title>
      <link>/2013/08/10/pku-summer-course2/</link>
      <pubDate>Sat, 10 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/08/10/pku-summer-course2/</guid>
      <description>
        

&lt;h3 id=&#34;前言&#34;&gt;前言&lt;/h3&gt;

&lt;p&gt;距上一篇时间颇长，不过继续Jiashun老师的讲课心得。上一篇谈到稀疏、弱信号的一种处理框架——Higher
Criticism，在分类、聚类等领域可以有比较好的应用。具体如何应用，此处不详谈，大家可以看看他第二节课的PPT以及该篇论文。在第二节课结束时，他提了一个结论：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Surprisingly, penalization methods (e.g., the L0-penalization method)
are not optimal for rare/weak signals, even in very simple settings
and even with the tuning parameters ideally set。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是说在稀疏、弱信号下，由L0衍生出来的方法并不是最优的，比较容易出问题。虽然我依稀记得某些论文模拟显示信噪比过低时候不少penalty方法结果并不太好，不过Jiashun老师的这个结论还是让我比较吃惊，毕竟被很正经的提出来了，而且他还有相对的解决方案！着实让我很感兴趣。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;h3 id=&#34;donoho的不确定原则-uncertainty-principle-与信号恢复&#34;&gt;Donoho的不确定原则（Uncertainty Principle）与信号恢复&lt;/h3&gt;

&lt;p&gt;Jiashun老师说，关于信号恢复最早应该可以追溯到Donoho在1989年的论文&lt;strong&gt;Uncertainty
Principle and Signal
Recovery&lt;/strong&gt;。在这片文章中，Donoho给出了类似于海森堡测不准原理的不确定原则（UP）。海森堡测不准原理通俗来讲即微观粒子某些物理量不可能同时被精确测量准确，一个量越确定，另外一个量的不确定程度就越大。Donoho的不确定原则通俗来讲即，离散时间点
*t* = 0, 1, …, *n* − 1 有观测 &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;，做傅里叶变换有&lt;/p&gt;

&lt;p&gt;$$
\hat{Y}_w = \frac{1}{\sqrt{n}}\sum^{n-1}_{t=0}Y_t e^{-2\pi it/n}, \quad w = 0, 1, \ldots, n - 1
$$&lt;/p&gt;

&lt;p&gt;用 &lt;em&gt;T&lt;/em&gt; 和 &lt;em&gt;W&lt;/em&gt; 分别表示 &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; 和 $\hat{Y}_w$
中的非零的位置，那么就会得到一个不确定原则，&lt;/p&gt;

&lt;p&gt;$$
\|T\|\cdot\|W\| \geq n, \Longrightarrow \|T\| + \|W\| \geq 2\sqrt{n}
$$&lt;/p&gt;

&lt;p&gt;直观的解释和测不准原理类似，当时域 &lt;em&gt;T&lt;/em&gt;
上的非零点位置很稀疏时，那么关于频域 &lt;em&gt;W&lt;/em&gt;
上的非零点就不会稀疏，他们被一个 $2\sqrt{n}$
的下界给bound住了。也就是说*&lt;strong&gt;T*和*W*不能同时太稀疏&lt;/strong&gt;！虽然这可能是自然界非常普遍的规律，但是我有些疑惑，这个原则对于理解L0有什么意义？&lt;/p&gt;

&lt;p&gt;在信号恢复中有这么个结论，观测 &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;
可以完美地被两组基给表示出来跳点基（spikes）和正弦基（sinusoids）&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; = Spike(&lt;em&gt;t&lt;/em&gt;)+Sinusoid(&lt;em&gt;t&lt;/em&gt;),  *t* = 0, …, *n* − 1&lt;/p&gt;

&lt;p&gt;现假设一个无噪音的模型&lt;/p&gt;

&lt;p&gt;*Y* = *X**β* = ∑&lt;sub&gt;&lt;em&gt;γ&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;β&lt;/em&gt;&lt;sub&gt;&lt;em&gt;γ&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;ϕ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;γ&lt;/em&gt;&lt;/sub&gt;,  *p* = 2&lt;em&gt;n&lt;/em&gt;,  *X* = [&lt;em&gt;Φ&lt;/em&gt;, &lt;em&gt;Ψ&lt;/em&gt;]&lt;/p&gt;

&lt;p&gt;其中 &lt;em&gt;Φ&lt;/em&gt; 和 &lt;em&gt;Ψ&lt;/em&gt; 分别是两组基。
$$[\Phi, \Psi] = [\underline{\phi_1, \ldots, \phi_n}, \underline{\phi_{n+1}, \ldots, \phi_{2n}}]$$
，此处令&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;*T* ⊂ {1, 2, …, &lt;em&gt;n&lt;/em&gt;}, 为“时域”，&lt;/li&gt;
&lt;li&gt;*W* ⊂ {*n* + 1, …, 2&lt;em&gt;n&lt;/em&gt;}, 为“频域”，&lt;/li&gt;
&lt;li&gt;要找到 &lt;em&gt;β&lt;/em&gt; 非零的位置，且具有稀疏性
$\\|T\\| + \\|W\\| \ll n$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目标就是给定 (&lt;em&gt;X&lt;/em&gt;, &lt;em&gt;Y&lt;/em&gt;)，来恢复稀疏的 *β*，此时根据Occam&amp;rsquo;s
Razor的原则，我们相信真实的 &lt;em&gt;β&lt;/em&gt; 应该是最稀疏的。转化为具体形式就是
ℓ&lt;sub&gt;0&lt;/sub&gt; 惩罚，&lt;/p&gt;

&lt;p&gt;(&lt;em&gt;P&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt;): min∥*β*∥&lt;sub&gt;0&lt;/sub&gt;,  such that   *Y* = *X*&lt;em&gt;β&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;虽然基于Occam&amp;rsquo;s Razor原则，那么 ℓ&lt;sub&gt;0&lt;/sub&gt;
的稀疏解是不是唯一的呢？答案是唯一的。之前的UP原则已经暗含了：&lt;strong&gt;对于*Y* = *X&lt;/strong&gt;β*，不可能同时存在多个稀疏解.**&lt;/p&gt;

&lt;p&gt;当然这个唯一性也是有条件的，当&lt;/p&gt;

&lt;p&gt;$$
\|T\| + \|W\| &amp;lt; \sqrt{n}
$$&lt;/p&gt;

&lt;p&gt;时，(&lt;em&gt;P&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt;) 会有唯一解，ℓ&lt;sub&gt;0&lt;/sub&gt;
惩罚是最优的。如何理解这个upper bound的条件呢？&lt;/p&gt;

&lt;p&gt;直观理解，结合不确定原则，时域与频域上非零稀疏个数至少是
$2\sqrt{n}$个，结合UP的那个乘法与加法的不等式，稀疏可以定义为&lt;/p&gt;

&lt;p&gt;$$
\|T\| + \|W\| = 2\sqrt{n} \quad \text{and} \quad \|T| = \|W\| = \sqrt{n}
$$&lt;/p&gt;

&lt;p&gt;如果加上个两者之和最多少于 $\sqrt{n}$
个，也就说明了时域、频域不能同时太稀疏，此时只有一个域上稀疏，那么这种恢复是唯一的。&lt;/p&gt;

&lt;p&gt;如果要证明，也比较简单，形式推论如下：假设同时存在两个稀疏解
&lt;em&gt;β&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; 和 &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;，那么做差
*σ* = &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; − &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; 也是一组稀疏解对于
0 = *X**σ*，同样要满足UP条件，那么就有&lt;/p&gt;

&lt;p&gt;$$
\begin{split}
2\sqrt{n} \leq &amp;amp; \|T(\sigma)\| + \|W(\sigma)\| \&lt;br /&gt;
\leq &amp;amp; \|T(\beta_1)\| + \|T(\beta_2)\| + \|W(\beta_1)\| + \|W(\beta_2)\| \&lt;br /&gt;
&amp;lt; &amp;amp; \sqrt{n} + \sqrt{n} = 2\sqrt{n}
\end{split}
$$&lt;/p&gt;

&lt;p&gt;可以看到与条件矛盾，那么也就是说不会存在两组稀疏解，ℓ&lt;sub&gt;0&lt;/sub&gt;
的解是唯一的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;正是由于有这样的结论，所以一直以来我们都相信基于 ℓ&lt;sub&gt;0&lt;/sub&gt;
的稀疏解是最优的，但是我们可能忽略了模型一个关键的假设：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;模型为无噪音，或者是信噪比很高，噪音影响很小&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以回到稀疏、弱信号的场景，我们就会很有理由怀疑基于 ℓ&lt;sub&gt;0&lt;/sub&gt;
惩罚的方法以及相关的衍生的ℓ&lt;sub&gt;1&lt;/sub&gt;惩罚的解是最优的吗？如果不是，如何做才可以处理这种稀疏、弱信号。&lt;/p&gt;

&lt;p&gt;首先我们阐述Occam&amp;rsquo;s
razor对于稀疏弱信号尝试是不太合适的。Jiashun老师给出了一个图形很好的阐述所存在的问题：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://1xkrlw.bn1301.livefilestore.com/y2pfonBhhZDcNwe2c8N8kdiKFMCOvjY7k1McuLh-SRVgEk-oV9KN9HIi0jQ7OABSNI-XN9jUEeDNVOIcCOpwzlYKP9bYs6jcfdgnytqvBp2p2c/occam.png&#34; alt=&#34;occam&#34; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到，如果没有噪音，或者噪音比较小时，恢复较大的真实的信号是很好的，不过当信号很弱，信噪比比较高时候，那么这些真实信号就会被一些噪音包围，而我们也无法分辨出来。用Occam&amp;rsquo;s
razor去挑选最简单的，显然效果会比较差。&lt;/p&gt;

&lt;p&gt;另外一个问题，对于稀疏、弱信号，根据之前的Phase
Diagram的信号恢复区域划分，以及上面这个图形，可以看到精确恢复已经是不可能了，那么用如下定义的*Oracle
property*也就不再合适：&lt;/p&gt;

&lt;p&gt;$$\min_{\beta} \,\, P(\hat{S} \neq S(\beta))$$&lt;/p&gt;

&lt;p&gt;即找出来的非零的系数与真实非零的系数（信号）是同一个的概率非常高，趋近于1。可以看到这种损失函数对于并不强调对于弱信号的发掘，只要大部分强信号找到了就差不多了。一个更为直接的损失函数定义可能是所谓的Hamming距离&lt;/p&gt;

&lt;p&gt;$$\min_{\beta} \,\, \text{Hamm}_p(\hat{\beta}, \beta) = \sum^p_{i = 1}(\text{sgn}(\hat{\beta}_i) \neq \text{sgn}(\beta_i))$$&lt;/p&gt;

&lt;p&gt;由于关注所有系数（系数为0和系数非0）的损失，所有强、弱和无关信号都纳入了优化的目标中，尽量不误估系数为0的和系数非0的，所以要求比Oracle
property更为严苛，希望不放过一丝蛛丝马迹。但是问题也将会变得更为复杂了！&lt;/p&gt;

&lt;p&gt;讨论了这么多，费了很大劲说明之前深以为然的 ℓ&lt;sub&gt;0&lt;/sub&gt;
对于稀疏、弱信号不再是fundamentally
correct了，那接下来有什么办法可以解决呢？推翻旧王朝相对来说还容易点，重新建个新王朝却更难了。Jiashun老师研究了那么久，自然有些办法来处理的。&lt;/p&gt;

&lt;h3 id=&#34;graphlet-screening&#34;&gt;Graphlet screening&lt;/h3&gt;

&lt;p&gt;Jiashun老师开头先讲了个故事，说有次Fan他们来他们那里做分享，很开心的分享他们关于screening、SIS等方面的工作，Jiashun老师自然先不了解Fan做的SIS和ISIS工作，对于screening也构想了自己的方法，但是当他听完Fan老师讲完他的思路后，他大腿一拍，“这不和我的思路一个样嘛！”&lt;/p&gt;

&lt;p&gt;如果诸君了解Fan老师在screenning方面的工作，那么就不会陌生Jiashun老师的思路。与Fan老师的SIS、ISIS思路相同，对于高维问题分两步走，一步screening得到一部分变量，然后对这些变量用些比较精细的方法诸如SCAD惩罚再来挑选，再稍微对第二步迭代调整下，可移除那些假非零系数（假信号）。而Jiashun老师的变量选择的方案也是一个两步方法。&lt;/p&gt;

&lt;p&gt;对于细节此处不详述，我只是谈谈理解。Jiashun老师有两个很重要的想法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;相关性不是噩梦，对于估计可能会降低估计效率（相当于样本减少），但是对于信号检测，却是是一个福音。我举个例子来理解，比如舆情监测，在浩如烟海的网络上有个人喊了句“打到XX”，这信号弱的简直没人知道，但是如果有几个与这个人存在某种关系的人，他们也跟着喊了句“打到XX”，虽然都很弱，但是还是起到了增强信号的作用，相当于几个弱信号汇集一起，让其中一个人发出了怒吼“打到XX”，那么自然这个时候信号检测相对会容易些，而边上的几个弱信号也顺便被检测到。因此要善于利用相关性来增强信号，对于稀疏、弱信号是一个很重要的手段。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;信号稀疏，变量之间的相关性也是稀疏的。将变量间的相关性看做一个图，也就是说信号会分散到很多小子图中，而且这些小子图内部相关性强，而相互之间没什么联系。用形象的话来说，就是信号分散于很多小岛屿上，具体哪些小岛屿有信号还不太清楚，但是我知道信号就分散这些小岛上，小岛上处处散发着信号的微光，而小岛屿之间对另外一个信号的发现没有任何帮助。好比中国大地星星之火隐约可见，却不知他们各自藏身于何处。Jiashun老师对变量间的相关性用一个阈值来控制（screening），以达到图的稀疏（sparse）和seperable（分离）的效果。我个人觉得这个假设有点强，将大图划分为多个独立的子图，这种方法略显有些暴力啊，你想这些星星之火间真的没有联系么？好吧，就当没有什么联系吧，因为我也没啥特别好的想法:(&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此整体思路就是：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;假设变量形成的图 𝒢 是稀疏的，信号可分解为多个独立子图
𝒢&lt;sub&gt;&lt;em&gt;s&lt;/em&gt;&lt;/sub&gt;（最大连通子图）。于是利用相关性对原来的观测进行变化，通过这种方式增强信号后对图𝒢
进行screening，即可获取支撑集，然后分解为多个子 𝒢&lt;sub&gt;&lt;em&gt;s&lt;/em&gt;&lt;/sub&gt;；&lt;/li&gt;
&lt;li&gt;在这些子图上，再利用加惩罚的MLE方法来估计、拟合即可。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;稍微细节点阐述如下&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;首先有个概念称作Graph of Strong
Dependence(GOSD)，即有图𝒢 = (&lt;em&gt;V&lt;/em&gt;, &lt;em&gt;E&lt;/em&gt;)，两个节点有边的条件是$|G(i, j) \geq \frac{1}{\log(p)}|$。因Gram
matrix
*G* = *X*′*X*稀疏可认为图𝒢稀疏。，从GOSD可以引出一个很关键的假设：𝒢&lt;sub&gt;&lt;em&gt;S&lt;/em&gt;&lt;/sub&gt;可以分解为许多不相连接的小块，每个小块都是最大连通子图，即上面的第二点重要的想法。不过图稀疏的，但并不意味着图的结构很简单，稀疏图的小块同样可以很复杂。然后定义*β*的支撑集
*S* = &lt;em&gt;S&lt;/em&gt;(&lt;em&gt;β&lt;/em&gt;)={1 ≤ *i* ≤ &lt;em&gt;p&lt;/em&gt;, &lt;em&gt;β&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; ≠ 0}
 由支撑对应的节点可以形成子图𝒢&lt;sub&gt;&lt;em&gt;S&lt;/em&gt;&lt;/sub&gt;。&lt;/p&gt;

&lt;p&gt;要估计支撑集的办法与SIS类似，肯定要粗暴点，不能用加惩罚类的最小二乘法（PLS）等来挑选，PLS做下一步的精挑和调整是比较合适的。Fan针对他的模型提出的思路是用边际似然来做对变量重要性排序然后挑选前面重要的变量，这个方法非常常见，没有什么特殊。不过前面讨论过，这些方法都是基于信号比较强的假设提出来的，多余rare/weak的信号是第一步screening就会将信号给踢出大门外，即使下一步再来调整，也基本无望重新找回来。&lt;/p&gt;

&lt;p&gt;在Screening步中，对于rare/weak信号的挑选此时就需要利用上面提到的第一点重要想法，利用相关性来筛选rare/weak信号。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Univariate Penalized Screening(UPS)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Jiashun老师提出假设简单模型
*Y* = *X**β* + &lt;em&gt;z&lt;/em&gt;,  *z* ∼ &lt;em&gt;N&lt;/em&gt;(0, &lt;em&gt;I&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;)
 UPS方法不是直接对X或Y做screening，而是对变化数据 $\tilde{Y} = X&amp;rsquo;Y$ 做
screening，这其实一种增强信号的处理，以便于挑选rare/weak信号，它与之前Jiashun老师提出
Innovated Higher Criticism 有很强的的关联，先阐述 Innovated HC
方法以便于理解，Innovated HC 也是一种对于 rare/weak
信号挑选比较合适阈值的方法。对于阈值选择问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;直接的想法就是用&lt;a href=&#34;http://joegaotao.github.io/cn/2013/07/PKU-summer-short-course/&#34;&gt;上一次提到的HC方法&lt;/a&gt;，假设各个信号独立无相关性来挑选信号，即
*β* ∼ &lt;em&gt;N&lt;/em&gt;(&lt;em&gt;μ&lt;/em&gt;, &lt;em&gt;I&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;)。但是实际情况多数各种信号噪音间有相关性
*Σ* ≠ &lt;em&gt;I&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;，因此直接用HC似乎不那么美好。&lt;/li&gt;
&lt;li&gt;另外一种想法就是常见的Whitening方法，用 &lt;em&gt;Σ&lt;/em&gt;&lt;sup&gt;−1/2&lt;/sup&gt;&lt;em&gt;β&lt;/em&gt;
线性变化使得变量相关性去掉
&lt;em&gt;Σ&lt;/em&gt;&lt;sup&gt;−1/2&lt;/sup&gt;*β* ∼ &lt;em&gt;N&lt;/em&gt;(&lt;em&gt;Σ&lt;/em&gt;&lt;sup&gt;−1/2&lt;/sup&gt;&lt;em&gt;μ&lt;/em&gt;, &lt;em&gt;I&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;)，然后再用HC方法来挑选。&lt;/li&gt;
&lt;li&gt;还有一种想法就是所谓的Innovated HC，应用HC于如下变化
&lt;em&gt;Σ&lt;/em&gt;&lt;sup&gt;−1&lt;/sup&gt;*β* ∼ &lt;em&gt;N&lt;/em&gt;(&lt;em&gt;Σ&lt;/em&gt;&lt;sup&gt;−1&lt;/sup&gt;&lt;em&gt;μ&lt;/em&gt;, &lt;em&gt;Σ&lt;/em&gt;&lt;sup&gt;−1&lt;/sup&gt;)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面的三种方法貌似都有一定道理，但是哪种最好呢？对于相关性，我们很多时候认为对估计是不太好的事情，但是对于信号检测，这却是有益的，上面我已经有阐述，所以直接HC会损失不少信息，那么后面两种方法看起来会更好些。当然看是看不出来的，简单计算下便可得出答案。&lt;/p&gt;

&lt;p&gt;假设 &lt;em&gt;μ&lt;/em&gt; 要不是为0，要不为信号 *τ* &amp;gt; 0，&lt;em&gt;Σ&lt;/em&gt; 是一个 2 × 2 分块矩阵
$$
\begin{pmatrix}
1 &amp;amp; h \&lt;br /&gt;
h &amp;amp; 1 \\
\end{pmatrix}
, \quad \|h\| &amp;lt; 1
$$&lt;/p&gt;

&lt;p&gt;稍微一个简单的矩阵计算，三种边际的信噪比（Marginal SNR）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;纯HC：&lt;em&gt;τ&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Whitening后HC：$[2/(1+\sqrt{1 - h^2})]\tau$;&lt;/li&gt;
&lt;li&gt;Innovated后HC：[1/(1 − &lt;em&gt;h&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;)]*τ*。
可以看到变换后，Whitening 和InnovatedSNR 都增加了，但是 Innovated
的增加的更多。用图形来阐述就是下图，左图是原始信号，而右边是经过变换后的信号，原弱信号增强了，当然边上的噪音也会有些增加了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://1xloxw.bn1304.livefilestore.com/y2pR8wojOtsWfBdyn7R_U3IM4yKTWmraFbWvktLKhGFTDUroalL1TzrTSLqrGsfkizxIeZoMSD_6g7u0BAYb3xdm6yDM01dfs6z3oJjTaSmxMU/SNR.png&#34; alt=&#34;innovated&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这种 Innovated HC 与 UPS 方法的联系在于 &lt;em&gt;X&lt;/em&gt;
是随机阵时，有一个完美的随机阵版本（Stein&amp;rsquo;s normal means
model）$X \overset{i.i.d}{\sim} (0, \frac{1}{n}\Omega)$,
而Stein正态均值模型有 *W* ∼ &lt;em&gt;N&lt;/em&gt;(&lt;em&gt;β&lt;/em&gt;, &lt;em&gt;Σ&lt;/em&gt;)，其中
*Σ* = &lt;em&gt;Ω&lt;/em&gt;&lt;sup&gt;−1&lt;/sup&gt;，于是乎 *X*′*Y* = *X*′*X&lt;strong&gt;β* + *X*′*z*，而
*X*′*X&lt;/strong&gt;β* ≈ *Ω&lt;strong&gt;β* 和 *X*′*z* ≈ &lt;em&gt;N&lt;/em&gt;(0, &lt;em&gt;Ω&lt;/em&gt;)，于是
*X*′*Y* ≈ &lt;em&gt;N&lt;/em&gt;(*Ω&lt;/strong&gt;β*, &lt;em&gt;Ω&lt;/em&gt;)，即Innovated
HC。所以UPS方法做Screening会保证一些较好的性质，如Sure
Screening（信号基本都在筛选出来的信号中），Separation After
screening(SAS)（存留的信号满足GOSD，可以拆分多个不相连的子块）。&lt;/p&gt;

&lt;p&gt;虽然这种UPS的方法虽然可以保证较好的性质，不过需要较强的条件，而且还会出现&lt;strong&gt;信号抵消（Signal
cancellation）&lt;/strong&gt;的现象。（该现象在&lt;a href=&#34;http://www.stat.cmu.edu/~roeder/publications/wr2009.pdf&#34;&gt;Wasserman，2009年论文的第五页&lt;/a&gt;举了一个小例子说明这个现象，主要原因就是相关性的介入，虽然真实信号还比较强，但相关性使得估计的信号被抵消而减弱使得screening时候信号被当做噪音给去掉了，导致False
Negative上升了。）这也是一个比较严峻的问题，会导致把许多真信号给删掉了。为了解决这个问题，我们就需要不仅仅只利用UPS，还需要利用之前的Gram
matrix *G* = *X*′&lt;em&gt;X&lt;/em&gt; 蕴含的稀疏图信息，尽量防止Signal
cancellation现象出现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Graphlet Screening（GS）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;一个直接而暴力的想法，就是扩展UPS，将单变量的screening变成多变量的screening，即多个变量一起满足某个阈值限制时才将这些变量选入，从单变量试到m变量的screening，如果某些变量一直出现，则将其认定为蕴藏了信号，留作下一步的cleaning。这个想法目的就是想消除&lt;strong&gt;Signal
cancellation&lt;/strong&gt;，看起来是*比较有希望*的，不过坏处也显而易见，计算量的急剧增加，将涉及至少$\binom{p}{m}$个子模型的计算，而且也很可能使得选出来的变量更难在下一步中将有用变量与无用变量分离开。因此想法虽然还不错，还是还需要打磨下，于是便有了Jiashun老师Graphlet
Screening的方法，改进的想法如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;只考虑需要考虑的有价值的的子模型，贯穿GOSD的想法到screening和cleaning中，只关注内部有强相关的子图，即仅利用Gram
matrix中的 &lt;em&gt;X&lt;/em&gt;
来大大削减所需要考虑的子模型个数；同时还能解决信号抵消的问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GOSD降低计算量是因为假设图是 &lt;em&gt;k&lt;/em&gt; -sparse的，这样就从原来的
&lt;em&gt;O&lt;/em&gt;(&lt;em&gt;p&lt;/em&gt;&lt;sup&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sup&gt;) 降到了差不多 &lt;em&gt;O&lt;/em&gt;(&lt;em&gt;k&lt;/em&gt;&lt;sup&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sup&gt;)
的级别。具体的定理证明可详见Jiashun老师的论文和slides，此处再稍微提下GS的算法过程：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;先利用GOSD获取稀疏图，然后在该图上选取子模型，每个子模型的节点数$_0不超过
&lt;em&gt;m&lt;/em&gt;
个，对这些子模型进行screening。由于每个节点于自身互通，于是最初的的子模型都是单节点。&lt;/li&gt;
&lt;li&gt;然后初始化留存节点集
𝒰&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;⋆&lt;/sup&gt;，对每个子模型地节点进行检验，是否要选入该子模型中的部分变量。想法是如果Y在整个该子模型
ℐ&lt;sub&gt;0&lt;/sub&gt; 变量上的投影的平方和，与在该子模型与留存节点集共有变量
$\hat{F}=\mathcal{I}_0 \cap \mathcal{U}^{\star}_p$
上的投影平方和，两者的差值如果大于某个阈值（经验性），则将该子模型的变量与留存变量集的差集变量选入，更新留存变量集（其实是一种适应性卡方检验）。该步骤的直观理解是如果两者的共有变量解释能力所占比重比较小，感觉像噪音，则说明另外一部分变量在该子模型中很可能是信号。&lt;/li&gt;
&lt;li&gt;screening后剩下的变量分解到各个自己的子模型
中ℐ&lt;sub&gt;0&lt;/sub&gt;，对这些子模型分别做类似 ℓ&lt;sub&gt;0&lt;/sub&gt;
变量选择既可挑选各个子模型的有用信号了。下面模型中的系数还有个阈值限制，需要大于&lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*g&lt;strong&gt;s*&lt;/sup&gt;。&lt;em&gt;P&lt;/em&gt;&lt;sup&gt;ℐ&lt;sub&gt;0&lt;/sub&gt;&lt;/sup&gt;
表示在子模型 ℐ&lt;sub&gt;0&lt;/sub&gt; 变量上的投影。
∥&lt;em&gt;P&lt;/em&gt;&lt;sup&gt;ℐ&lt;sub&gt;0&lt;/sub&gt;&lt;/sup&gt;(*Y* − &lt;em&gt;X&lt;/em&gt;&lt;sup&gt;⋆, ℐ&lt;sub&gt;0&lt;/sub&gt;&lt;/sup&gt;)*ξ*∥+(&lt;em&gt;u&lt;/em&gt;&lt;sup&gt;*g&lt;/strong&gt;s*&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;∥*ξ*∥&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整体看来，过程还是显得有些繁琐，引入了不少参数，这些阈值参数有些给出了解析式子，有些比如
&lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*g&lt;strong&gt;s*&lt;/sup&gt;
却比较难选择。算法给完后面免不了是不少理论性质的证明，在严苛的Hamming
loss上可以最优，比lasso好，Phase
diagram上表现也很好等等好的理论性质，不过这些我都没怎么细看了，一时看不明白也看不完呀！总之给我的印象就是还不错！不过遗憾在于相比HC框架的简约不用给代码，但是这个模型还是做个R包或者给点代码吧，践行Zou
Hui的&lt;/strong&gt;统计产品**理念看来还是有必要的:)&lt;/p&gt;

&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;

&lt;p&gt;总之，Jiashun老师的想法就是想尽可能地挖掘变量间的关系来帮助变量选择。之前Zou
hui的&lt;a href=&#34;http://pages.cs.wisc.edu/~shao/stat992/zou2006.pdf&#34;&gt;Adaptive
lasso&lt;/a&gt;，Bulhman的&lt;a href=&#34;http://arxiv.org/pdf/0808.1013.pdf&#34;&gt;Multi-step变量选择&lt;/a&gt;，Longzhe利用&lt;a href=&#34;http://bioinformatics.oxfordjournals.org/content/24/9/1175.full&#34;&gt;network做laplacian惩罚&lt;/a&gt;，还有相似的Cun-hui老师先学adjacency
matrix，然后&lt;a href=&#34;http://arxiv.org/pdf/1112.3450.pdf&#34;&gt;MCP+laplacian惩罚来做变量选择&lt;/a&gt;，以及今年在北京大学春季统计会议上Jinzhu老师提出的&lt;a href=&#34;http://arxiv.org/pdf/1208.5584.pdf&#34;&gt;preconditioning方法&lt;/a&gt;，用SVD的
&lt;em&gt;U&lt;/em&gt; 矩阵和 &lt;em&gt;D&lt;/em&gt; 矩阵来对 &lt;em&gt;X&lt;/em&gt;
做变换，也都是想对原始的变量进行挖掘，想办法在更弱的条件下来做更准确的变量选择，不过我更倾向于用图（变量关系）来提高变量选择的准确性，Witten和Tibshirani的&lt;a href=&#34;http://www-stat.stanford.edu/~tibs/ftp/WittenTibshirani2008.pdf&#34;&gt;Scout方法&lt;/a&gt;比较切合我的意图，不过现在看到Jiashun老师的思路和想法，不禁还是被深深吸引，在Gram
Matrix上和变量变换上做文章，想法很好很深刻，何况还有之前的HC框架呢！尽管论文比较难读，涉及很多性质证明，我也基本没怎么细看，但是我想如果有可能，还是希望能够在这个基础上再继续做点东西吧，但愿:)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>PKU暑期高维统计学习心得(I)</title>
      <link>/2013/07/19/pku-summer-course1/</link>
      <pubDate>Fri, 19 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/07/19/pku-summer-course1/</guid>
      <description>
        

&lt;h3 id=&#34;印象&#34;&gt;印象&lt;/h3&gt;

&lt;p&gt;为其两个周的北大关于高维统计的暑期课程即将告一段落，我回来奔跑了两周，身体略感疲惫，现在总算可以休息下，然后停下来消化下讲过的内容。&lt;/p&gt;

&lt;p&gt;这次来讲课的老师学术能力都很强，都是四大paper等身的青年学者。老师们讲课的风格不一，最好玩的当属Tiefeng
Jiang老师，他讲起课来就像说东北二人转，段子一个接一个，东北味的口音让我第一节课毫不犯困。而且深入浅出，随机矩阵这种比较数学的研究领域，也被他讲的比较好理解。不过后面由于有事情，以及之后的内容过于数学化，我就没有再跟下去了。Zhu
Ji老师讲的很细致，不过内容偏简单了，听了两节课后我也没有跟下去。Cun-Hui
Zhang老师做的很理论，深厚的数理分析功底，以及对高维问题理解的深刻让我感觉很敬畏，不敢靠近。对他后面做的scaled
lasso和LPDE的结果很感兴趣，想用来做点检验的试验，不过邮件找老师要代码现在还没有回复，略感伤心，看来只能过几天自己写了。Yang
Feng老师很年轻，在Fan老师那边做了很多非常好的工作，不过由于之前我看了不少Fan老师的东西，对他的讲的思路相对比较熟悉，也就没有太用心听而刷微博、做项目去了，真是一大罪过啊！&lt;/p&gt;

&lt;p&gt;整个课程中对统计所持的观点和态度，我最欣赏的是Hui Zou和Jiashun Jin老师。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;Hui Zou老师在变量选择、图模型做了不少很好的工作，比如现在很常用的elestic
net、adaptive lasso等，都是非常简约而好用的工具。Hui
Zou老师为人谦虚，对自己所做的东西不夸耀、不吹捧，他认为统计的工作更像是“完成一个产品”的工作，做出来的方法最好能做成软件包为人所用，而且还要比较好用，所以他的文章不少都会附上R包。这一点我很喜欢，统计本身就是一个应用的学科，如果做的过于数理，缺少实际的价值，并且算法写的没效率没法用，这些都是没法促成统计在现实生活大规模应用的。我觉得当前统计之所以这么热，也主要是当年统计从英国转入美国后，有了Tukey等人不断地大力推动数据分析的理念，推进一些有效的统计分析方法，才有了现在统计一片大热的局面和现在所谓大数据的时代。&lt;/p&gt;

&lt;p&gt;Zou
Hui老师还提倡多做实验，多种方法多做比较，不要限制于一种方法上。我深以为然。以前我学习统计的感觉就是一定要找一个方法完美的解决这个问题，和做数学问题样，做到一个唯一解。后面我逐渐的体悟到，统计面对的是数据，它本身就是具有随机性的，用多种方法来看这个数据虽然结果会有差异，也许某个方法表现比较好，但是不是说明这个方法在后面遇到了同类型的问题时候，在使用这个方法的效果就一定会好。就拿各种penalty的方法，真实数据你也不知道信噪比如何，回归系数是怎么样，也许模拟结果显示某某方法很好，超越了其他方法，但是面对真实数据，好的方法只是“概率性”地增加了我的信心，我无法确定scad一定比lasso分析的好，何况那些oracle性质只是概率意义上的呢，谁知道不会发生小概率事件并且后面Jiashun老师提到的rare/weak
signal问题更加增加了我对这些方法的恐慌。所以，做完理论后，回归到数据分析，唯一的办法就是多做比较，大胆假设，小心论证，发现共同的证据，这才是做统计和做数据分析的思维。&lt;/p&gt;

&lt;p&gt;整个暑期课程对我思维激发最大的是Jiashun
Jin老师的课程内容。由于课程进度有些快，加上这几天比较忙，我也没有研读老师paper，所以此处只是记录些大概想法，后面有时间会深入探讨。&lt;/p&gt;

&lt;h3 id=&#34;higher-criticism-and-rare-weak-signals&#34;&gt;Higher Criticism and Rare/Weak Signals&lt;/h3&gt;

&lt;p&gt;Jiashun老师讲关于稀疏、弱信号（rare/weak
signals）共三节课，最核心的是Higher Criticism and rare/weak
signals，然后还有就是关于变量选择的新思维。&lt;/p&gt;

&lt;p&gt;关于稀疏、弱信号，Jiashun老师认为在大p小n的情况下，有许多没有用的特征，当真实信号非常稀疏和微弱时，参数空间存在着一块&lt;strong&gt;不可能对参数进行很好推断&lt;/strong&gt;的区域。&lt;/p&gt;

&lt;p&gt;而导致信号过弱的情况，一个直接原因就是样本过少。信号强度以样本量存在一个2次的比例关系（一般CLT的速度）&lt;/p&gt;

&lt;p&gt;(signal strength)^2 ∝ *n* ∝ dollars or manpower&lt;/p&gt;

&lt;p&gt;这是一个很浅显的道理，增加样本（如果样本不是高度相关抽样所得），信号肯定会增强，但是很多情况下，随着样本增加，成本会大大提高，或者是维数又会大大增加，信号仍旧比较弱，那么此时如何去恢复或者估计呢？&lt;/p&gt;

&lt;p&gt;很多情况下，人们都认为他们的数据中信号是很强的，所以可以直接用那些高维的惩罚方法来恢复信号，或者认为强信号与弱信号之间存在巨大的鸿沟，他们可能没法互相转化，又或者认为信号很弱时，我们什么都不用干，因为什么方法都没用。一般来说，大海里捞针，信号本身确实挺弱的，要想寻找到这样的信号，确实是件非常难的事情。但是我们可以提出一个问题：什么样的情况下我们可以通过一些高维的方法找到这样的弱信号，在什么样的情况下我们又无法很好找到弱信号呢？如何量化这种信号可估和不可估的区域呢？&lt;/p&gt;

&lt;p&gt;Jiashun老师从FDR的弱点出发引出了自己的思路。&lt;/p&gt;

&lt;p&gt;对于简单的问题&lt;/p&gt;

&lt;p&gt;$$Y_i = \mu_i + \sigma z_i, \quad z_i \overset{iid}{\sim} N(0, 1), \quad i = 1, 2, \ldots, p$$&lt;/p&gt;

&lt;p&gt;如果只有很少量的信号&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;不为0，挑选信号的一个直接的方法就是用Wavelet
hard-thresholding，给出一个阈值&lt;/p&gt;

&lt;p&gt;$$
\hat{\mu}_i^H = \left \{
\begin{array}{lc}
y_i, &amp;amp; |y_i| \geq \sigma \cdot t \&lt;br /&gt;
0, &amp;amp; \text{otherwise}
\end{array}
\right.
$$&lt;/p&gt;

&lt;p&gt;这个时候选择阈值 &lt;em&gt;t&lt;/em&gt;
就是一个艺术化的工作，选大了会导致很多信号选不到，选小了就会导致很多噪音进来。一种选择阈值的方法即通过控制FDR水平（错误发现率），通俗的说，如果能使得选出来的信号中是假信号（噪音）的比例控制在一定水平之下，这样我们也是可以接受的，毕竟真信号还是选出来了，只是附带了一部分噪音罢了。想法是好的，但是实际中，用FDR控制阈值很可能选不到任何信号，因为我们期望FDR能有效果是基于一个信念：信号虽然稀疏，但是还是&lt;strong&gt;强（strong）&lt;/strong&gt;的，所以我们也许还是可以找到个相对好的阈值
&lt;em&gt;t&lt;/em&gt;
来找到强信号。但是现实中如果信号是弱的，信噪比比较高时，用FDR报告出来的信号便很可能是假的，因此控制FDR还是无法到达选较好阈值的目的。按Jiashun老师的话说，FDR其实与阈值选择没有太大关系，两个不太一样的目标。&lt;/p&gt;

&lt;p&gt;于是Jiashun老师从检测稀疏混合分布（Detection of sparse
mixtures）出来来导出他的想法和框架，与FDR有些类似，但是效果却大不相同。&lt;/p&gt;

&lt;p&gt;做如下假设检验：&lt;/p&gt;

&lt;p&gt;$$
\begin{array}{lr}
H_0 : X_1 \overset{iid}{\sim}N(0, 1), &amp;amp; 1 \leq i \leq p \&lt;br /&gt;
H_1^{(p)}: X_i \overset{iid}{\sim}(1 - \epsilon_p)N(0, 1) + \epsilon N(\tau_p, 1), &amp;amp; 1 \leq i \leq p
\end{array}
$$&lt;/p&gt;

&lt;p&gt;原假设即各变量是噪音，备择假设是各变量是一种噪音与信号的混合。其中参数有如下形式&lt;/p&gt;

&lt;p&gt;$$
\begin{array}{lc}
\epsilon_p = p^{-\beta}, &amp;amp; 0.5 &amp;lt; \beta &amp;lt; 1 \&lt;br /&gt;
\tau_p = \sqrt{2r\log p}, &amp;amp; 0 &amp;lt; r &amp;lt; 1
\end{array}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当 &lt;em&gt;ϵ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt; 很小时，比如 $\epsilon_p \ll 1/\sqrt{p}$
时，意味着只有极少的非零均值，此参数刻画着信号稀疏性（ &lt;em&gt;β&lt;/em&gt;
越大，&lt;em&gt;τ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt; 越小，信号越越稀疏）；&lt;/li&gt;
&lt;li&gt;当 &lt;em&gt;τ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;
比较小时，信号相对比较弱，此参数刻画着信号的强弱（&lt;em&gt;r&lt;/em&gt;
越大，信号越强）；一般 $\tau_p &amp;lt; \sqrt{2 \log p}$
时，信号就凑合能用了（only moderate significance）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于两个分布的检验（上述参数固定时候），Neyman-Pearson检验最优。那么自然我们就想通过似然比检验来刻画上述参数(&lt;em&gt;β&lt;/em&gt;, &lt;em&gt;r&lt;/em&gt;)不同区间的检验效力了。于是就有了如下非常惊艳的有关信号检测的Phase
Diagram&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://1xlhga.bn1301.livefilestore.com/y2pYbzYr-kSidbDba1eluxy8DunNQ_Y35tLh0_5rAwa-8PlVJ5Gmy9vd1UUa1Syw-V5hcJkn2ESY86DcDzY-7679-JjHlX4xabIvyb8mO7LCCw/hc.png&#34; alt=&#34;hc&#34; /&gt;&lt;/p&gt;

&lt;p&gt;此处划分了四个区域：可精确恢复（exact recovery）；几乎能全恢复（almost
full
recovery）；可检测的（detectable）；不可检测的（undetectable）。这些都是概率的语言，表示的概率强度不同。横坐标
&lt;em&gt;β&lt;/em&gt; 越大表示信号越稀疏，纵坐标 &lt;em&gt;r&lt;/em&gt;
越大表示信号越强。很多理论的结论都是在 *r* &amp;gt; 1
时的结论，也就是信号很强的时候咋算都会又不错的估计效果。右图是将横纵坐标都限制在
(0, 1)
区间中，而这一块也正是我们感兴趣的地方，信号稀疏而且很弱的时候估计效果如何？经过一些与检验相关的计算，这些曲线是可以直接算出来的，可以刻画可检测、不可检测、可估计的区域范围。&lt;/p&gt;

&lt;p&gt;我觉得这是一个非常能激发思维的结论。对于不可检测的区域，过于稀疏和过于弱的信号，尝试努力恢复的性价比是非常低的，几乎不可能；对于可估（estimatable）的区域，用现在常见的penalty方法基本可以做到比较好的恢复，能够分离开信号与噪音；但是对于可检测（detectable）的区域，虽然我们知道那里面有信号，但是几乎不可能将它们与噪音区分开（FDR失效），不过如果是做信号检测、分类、聚类等工作，进行有效的推断还是仍然有可能的。此时进行推断的框架不是FDR，而需要一个对稀疏、弱信号更敏感的框架，它有个响亮的名字——Higher
Criticism。&lt;/p&gt;

&lt;p&gt;Higher Criticism，我直译为为高阶鉴别法，Jiashun老师说始于Tukey
1976年Stat
411课程讲义笔记，大师的思维光芒真是能穿越历史呀。Jiashun老师推导的HC与Tukey的略有不同，更为一般化，式子如下：&lt;/p&gt;

&lt;p&gt;$$
HC^{*}_p = \max_{0 \leq \alpha \leq \alpha_0}\big\{
\sqrt{p}\big[\frac{\text{fraction significant at }\alpha - \alpha}{\sqrt{\alpha(1 - \alpha)}}\big]
\big\}
$$&lt;/p&gt;

&lt;p&gt;其中 &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt;
可以是1/2或1。一眼就可以看出来，这是一个比例估计的检验——分子是在控制 &lt;em&gt;α&lt;/em&gt;
水平时实际个体显著的比例与真实比例 &lt;em&gt;α&lt;/em&gt; 的差异；分母将 $\sqrt{p}$
拿到分母下就是比例 &lt;em&gt;α&lt;/em&gt; 的方差。那么这么做的含义是什么？&lt;/p&gt;

&lt;p&gt;仔细想下，这蕴含着一个二阶显著检验问题（second-level significance&lt;/p&gt;

&lt;h1 id=&#34;testing-想要知道在哪个水平下-我们检验的显著个体是真实-如果只看在某个水平下是否显著-一阶证据-然后依此证据来寻找显著个体其实并不十分理智-比如做了250次独立的检验-有11个在5-水平下显著-实际期望的平均显著个数是250-0-05&#34;&gt;testing）。想要知道在哪个水平下，我们检验的显著个体是真实，如果只看在某个水平下是否显著（一阶证据），然后依此证据来寻找显著个体其实并不十分理智。比如做了250次独立的检验，有11个在5%水平下显著，实际期望的平均显著个数是250*0.05&lt;/h1&gt;

&lt;p&gt;12.5个，也就是说在原假设为真的情况——假信号（噪音），也会有12.5个会显著。而11个与12.5个有差距很小的，所以我们很有理由怀疑这11个显著的信号不是是真信号，而很可能都是噪音。如果实际显著个数比期望显著个数大很多，那么我们可能更愿意相信在该显著水平
&lt;em&gt;α&lt;/em&gt; 下，真能会发现不少的信号。所以我们的目标就是想要调
*α*，看哪个水平下，这个HC值最大，这时候我们可以认为在这个水平下，我们可以发现信号，是可以检测的。&lt;/p&gt;

&lt;p&gt;Jiashun老师说，HC值对强信号、弱信号检测都非常敏感，而FDR仅对强信号敏感。我粗浅地想可能就是HC值基于p值后又做了一次检验的缘故吧。由于没有去做Jiashun老师留下的作业，所以理解还不深刻。后面还是回头再算算Phase
Diagram中的边界曲线来加深理解。&lt;/p&gt;

&lt;p&gt;Higher Cirticism实施比较简单，过程与FDR过程很类似。步骤如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;对每个特征都算一个z-score，然后根据z-score算个p值，&lt;/li&gt;
&lt;li&gt;对p值排序：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;π&lt;/em&gt;&lt;sub&gt;(1)&lt;/sub&gt; &amp;lt; &lt;em&gt;π&lt;/em&gt;&lt;sub&gt;(2)&lt;/sub&gt; &amp;lt; ⋯ &amp;lt; &lt;em&gt;π&lt;/em&gt;&lt;sub&gt;(&lt;em&gt;p&lt;/em&gt;)&lt;/sub&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;计算第*k*个HC值，也相当于算了一个z-score：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$HC_{p, k} = \sqrt{p}\big[\frac{\frac{k}{p}-\pi_{(k)}}{\sqrt{\pi_{(k)}(1 - \pi_{k})}}\big]$$&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;取最大值，计算相应的 *H*&lt;em&gt;C&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt;
值，找到对应的 *k*，前 &lt;em&gt;k&lt;/em&gt; 可以认为是真显著的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;*H&lt;strong&gt;C*&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt; = max&lt;sub&gt;1 ≤ *i* ≤ &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; ⋅ &lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;{*H&lt;/strong&gt;C*&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;, &lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;}&lt;/p&gt;

&lt;p&gt;对应着下面的图形大约可以可以理解这个过程，横轴是实际比例
&lt;em&gt;k&lt;/em&gt;/*p*，目的就是找到一个阈值，可以帮助我们检测到信号。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://1xisda.bn1302.livefilestore.com/y2p3dEywCYLZ4Hy1Rhs8uoicppH9hvrwVBdCUUOcFudDC67x1_Y-uzdxE-EVvZkhs3IxHnNAmVVFJ42GbdN9vhRIl_7J4uUxIysP9Ctt9AtePI/hc1.png&#34; alt=&#34;hc1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;然后Jiashun老师给出了他在2004年和他导师Donoho的一篇论文的结果，证明了
*H*&lt;em&gt;C&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt; 有最优的适应性(adaptivity)，证明
$HC^{\ast}_p &amp;gt; \sqrt{4\log\log(p)}$
时，可以获得犯第一类错误与第二类错误之和趋近于0。&lt;/p&gt;

&lt;p&gt;Higher
Criticism在宇宙学、天文学、基因、异常检测中研究比较多，因为那里的信号比较稀疏和弱，常规的方法已经不能满足需求。另外，HC非常适用于高维的screening、signal
detection、classification、clustering等方向，用HC来控制screening中的阈值，比常规的CV、FDR等方法提供了一个新的角度，并且简单有效，无需调参，理论性质也挺好。&lt;/p&gt;

&lt;p&gt;P.s.
一不小心突然发现写太多了，本只是记录下心得，不过写着写着觉得还是要重新捋一捋思路才行。之所以写这么多，很大原因是我对penalty太细节化的讨论感到有些厌倦了，里面谈到的统计思想性的东西并不多，所以Jiashun老师东西对我来说比较新颖，便一下子记录了不少，以留作后续继续研读。&lt;/p&gt;

&lt;p&gt;Jiashun老师后面还回顾了L0方法的本源，然后说明了在在稀疏、弱信号下，基于L0而衍生的一系列penalty方法都存在比较大的问题。这个论证让我感觉耳目一新，留作下篇再续。希望后面能在深入了解下Jiashun老师的工作，能够有更深的理解，能跳出当前的状态，既能看到他的方法好处，也能看到他的方法的弱点所在，因为我相信没有一种方法是万能的，总会有不完美的地方。&lt;/p&gt;

&lt;h3 id=&#34;送别&#34;&gt;送别&lt;/h3&gt;

&lt;p&gt;总之，两个星期的课程悄然结束。最后Yang
Feng老师说希望大家有一个欢乐的暑假时，我才意识到课程真的已经结束。这也意味着陪同我一起上课好朋友兼极客同志——小南，晓矛师弟、赛姐师妹即将离开北京，各自踏上自己的征途。回家写R包的写R包，远赴米国读博的读博，而我，还要坚守在北京，继续着前进的道路。其实感慨良多，因为研一这一年经历了不少心理的改变，尤其是2013年。不过，无论做什么，就全力以赴吧。遥祝晓矛、赛姐米国修炼过程顺利顺心，早日学成归来。祝小南同志潜心修道，将学术理论进行到底，早日成为一名极客+理论家。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://pan.baidu.com/share/link?shareid=2717730273&amp;amp;uk=771858297&#34;&gt;本次暑假课程的PPT（最后一天的还没有）加我下载的相关的论文在此了&lt;/a&gt;，愿喜欢这块内容的诸君好好学习！&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>高维变量选择问题的一点总结</title>
      <link>/2013/07/05/high-dim-summary/</link>
      <pubDate>Fri, 05 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/2013/07/05/high-dim-summary/</guid>
      <description>
        &lt;p&gt;近十年来，不管是统计界还是计算机界，高维数据问题依然是最热的话题。大数据时代带来的不仅仅是海量的数据，更多的是数据的复杂性和维度的多样性。对于一个个体的描述，我们不再仅仅是通过几个体征属性来描述，并试图通过一些方法来对个体做推断。伴随着互联网用户行为数据的收集、个体生理指标的测量，纵向数据、函数型数据、Tensor型数据大量涌现，描述个体的角度变得丰富多彩起来，我们总希望从这些复杂的关系中拨开云雾，发现数据间千丝万缕的关系，进而推断因果、预测未来。我不知道后面几十年这个目标是否可以实现，但是从目前的进展看，似乎还是有希望的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://pic.biodiscover.com/files/u/6v/201303081346539541.jpg&#34; alt=&#34;gene&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在这个背景下，我也赶了潮流，这一年基本都浸淫在高维问题中，也算熟悉了其中的一些理论。不过看多了高维问题也确实让人感到厌倦和烦躁，总感觉外表很酷，头脑干瘪，丧失了不少统计的味道。相反，有时简洁有趣的数据分析反而让人感觉更舒服，毕竟具体、实际的问题相比到处泛滥的理论更吸引人。&lt;/p&gt;

&lt;p&gt;目前我的这些总结还不多，很多东西都记录的是自己感兴趣的部分。其中主要沿着Fan 2001年的关于Concave Penalty的思路以及后续的发展进行了总结。他的文章对Penalty的问题讨论的很细致很深刻，2013年Fan又进一步提出了Folded Concave Penalty的一些性质，发展了一个较为统一的框架，结合2008年Zou提出的LLA算法，Concave Penalty的求解问题也变得快速，不需要Lasso问题的那些限制条件也具有良好的理论性质。不过正如Fan自己所说，如果没有之前对Lasso问题的详细讨论，LLA算法也不会提出来，虽然与LQA只是一个泰勒展开阶数的差别，但是实际效果却大不一样。潮流总在变化，估计Fan也是不甘心他的思路被现在各种五花八门的Penalty给淹没了吧。&lt;/p&gt;

&lt;p&gt;总结文档在&lt;a href=&#34;https://github.com/joegaotao/highDim/blob/master/highDim.pdf&#34;&gt;Github&lt;/a&gt;上，欢迎补充Lasso方面的内容，后续我会添加一些关于高维矩阵方面的心得。&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
